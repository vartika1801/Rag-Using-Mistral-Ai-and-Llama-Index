# RAG Application

This project demonstrates a **Retrieval-Augmented Generation (RAG)** application built using **Mistral AI** and **LlamaIndex**. It allows users to query content from a **technical PDF on Machine Translation with Attention** and receive precise, context-aware answers generated by a large language model.

---

## Features

- **PDF Ingestion:** Automatically loads and parses the machine translation PDF for content retrieval.  
- **Embedding & Vector Indexing:** Uses **Instructor** or **LangChain embeddings** to create a semantic vector index for fast and accurate retrieval.  
- **Mistral AI LLM:** Generates context-aware answers using the **Mistral-7B-Instruct** model.  
- **RAG Pipeline:** Combines vector retrieval with generative LLM responses for coherent answers.  
- **Chunking & Context Management:** Splits documents into manageable chunks (e.g., 1024 tokens) for efficient processing.

---

## Tech Stack

- **LLM:** Mistral-7B-Instruct  
- **Vector Index & RAG:** LlamaIndex v0.14.5  
- **Embeddings:** InstructorEmbedding or LangChainEmbedding (HuggingFace-compatible)  
- **Vector Store (optional):** FAISS, ChromaDB, or other persistent stores  
- **PDF Parsing:** SimpleDirectoryReader from LlamaIndex  

---

## Installation

```bash
# Install LlamaIndex and embeddings
pip install llama-index llama-index-llms-huggingface llama-index-embeddings-instructor llama-index-embeddings-langchain

